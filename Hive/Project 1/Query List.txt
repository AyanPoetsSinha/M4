--concatenate all files
cat file1.txt file2.txt file3.txt > file4.txt


--copy the file inside the HDFS

hdfs dfs -copyFromLocal './file4.txt' '/user/root/'

---then start hive

$hive

--create db

create database starwars;

--use db

use database starwars;


--create table under that db

> create table starwars
    > (actor STRING, Dialogue STRING)
    > ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
    > TBLPROPERTIES ("skip.header.line.count"="1");

---load the file into the table

hive> LOAD DATA LOCAL INPATH
    > './file4.txt'
    > into table starwars;

--- select all detailsf 

select actor,count(actor) as counter from starwars group by actor order by counter DESC;

---find the out put diretory

hive> dfs -ls 'hdfs:///user/root/output/export';

--- copy the output inside HDFS

 INSERT OVERWRITE DIRECTORY '/user/root/output/export/'
    > ROW format delimited fields terminated by '\n'
    > select actor,count(actor) as counter from starwars group by actor order by counter DESC;


---copy the output inside local

hive> dfs -copyToLocal 'hdfs:///user/root/output/export/000000_0' './output.txt';


--get out of hive

exit;

-- check the output file found in local

cat output.txt



